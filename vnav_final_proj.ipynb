{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"73adaac40a2549e8b22a988ae96e9e71","deepnote_cell_type":"text-cell-h1"},"source":"# Flying a Drone Through a Pendulum Hoop","block_group":"97c3fb1f554b431bb1d1546358c20720"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"27acdbfb563147459a03435ed82adf8a","deepnote_cell_type":"text-cell-p"},"source":"When dispatching autonomous drones into dynamic situations, users need to be able to trust that their drones can dynamically track objects in their surroundings and update their desired positions accordingly. In this project, we aim to model a simplified version, but simulating a swinging pendulum and a drone which must use the dynamics of the swinging pendulum-hoop and real-time object tracking to update its estimated position of the system and drive so it passes through the hoop at the perfect time. This project requires a combination of motion planning, perception, and control, adding a new level of challenge to existing work. ","block_group":"4298e0d3c7324bde9fd79e6d81a88505"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ede8c464aa64425f9ba4633c701ca957","deepnote_cell_type":"text-cell-p"},"source":"Currently, there are no pre-existing methods driving a drone through a swinging pendulum. There are two major components to this project: determining the drone-pendulum intersection point, and controlling the drone to approach that point at the correct time.","block_group":"e1f5faa30a254cc2a88254f6aaa41e04"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1702718978262,"execution_millis":7237,"deepnote_to_be_reexecuted":false,"cell_id":"82a2f7c6f69e437fabfc3fcd6f80da2f","deepnote_cell_type":"code"},"source":"import math\nimport cv2\nimport pydot\nimport matplotlib.pyplot as plt\nimport mpld3\nimport numpy as np\nimport os\nfrom IPython.display import HTML, display, SVG\n\nfrom pydrake.all import (\n    AddMultibodyPlantSceneGraph,\n    ControllabilityMatrix,\n    DiagramBuilder,\n    Linearize,\n    LinearQuadraticRegulator,\n    MeshcatVisualizer,\n    MultibodyPlant,\n    Parser,\n    StartMeshcat,\n    Propeller,\n    PropellerInfo,\n    RigidTransform,\n    RobotDiagramBuilder,\n    Saturation,\n    SceneGraph,\n    Simulator,\n    StartMeshcat,\n    WrapToSystem,\n    namedview,\n    VectorLogSink,\n    VectorSystem,\n    wrap_to,\n    MultibodyPositionToGeometryPose,\n    ModelInstanceIndex,\n    RenderCameraCore,\n    ClippingRange,\n    DepthRange,\n)\nfrom pydrake.all import RigidTransform, AddMultibodyPlantSceneGraph, DiagramBuilder, Simulator, MeshcatVisualizer, MeshcatVisualizerParams, SceneGraph, LeafSystem, Diagram, AbstractValue\nfrom pydrake.all import MultibodyPlant, Parser, FindResourceOrThrow, Meshcat, RigidTransform, MakeRenderEngineVtk\nfrom pydrake.examples import (\n    AcrobotGeometry,\n    AcrobotInput,\n    AcrobotPlant,\n    AcrobotState,\n    QuadrotorGeometry,\n    QuadrotorPlant,\n    StabilizingLQRController,\n)\nfrom pydrake.solvers import MathematicalProgram, Solve\nfrom pydrake.geometry import (\n    MeshcatVisualizerParams,\n    Role,\n)\nfrom underactuated import ConfigureParser, running_as_notebook\nfrom underactuated.meshcat_utils import MeshcatSliders\nfrom underactuated.quadrotor2d import Quadrotor2D, Quadrotor2DVisualizer\nfrom underactuated.scenarios import AddFloatingRpyJoint\n\nfrom copy import deepcopy\nfrom pydrake.examples import PendulumGeometry, PendulumParams, PendulumPlant\n\nfrom underactuated import running_as_notebook\n\nfrom pydrake.common import temp_directory\nfrom pydrake.geometry import StartMeshcat\nfrom pydrake.math import RigidTransform, RollPitchYaw\nfrom pydrake.multibody.parsing import Parser\nfrom pydrake.multibody.plant import AddMultibodyPlantSceneGraph\nfrom pydrake.systems.analysis import Simulator\nfrom pydrake.systems.framework import DiagramBuilder\nfrom pydrake.visualization import AddDefaultVisualization, ModelVisualizer\n\nimport dataclasses as dc\nfrom functools import partial\nimport os\nimport sys\nimport typing\n\nimport torch\nimport torch.utils.data\nimport torchvision\nimport torchvision.transforms.functional as Tf\n\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nimport numpy as np\nfrom math import atan2, cos, sin, sqrt, pi\n\nimport time\n\nfrom pydrake.all import (\n    AbstractValue,\n    Adder,\n    AddMultibodyPlant,\n    ApplyLcmBusConfig,\n    ApplyMultibodyPlantConfig,\n    ApplyVisualizationConfig,\n    BaseField,\n    CameraConfig,\n    CameraInfo,\n    Demultiplexer,\n    DepthImageToPointCloud,\n    Diagram,\n    DiagramBuilder,\n    DrakeLcmParams,\n    GetScopedFrameByName,\n    IiwaCommandSender,\n    IiwaDriver,\n    IiwaStatusReceiver,\n    InverseDynamicsController,\n    LeafSystem,\n    LcmPublisherSystem,\n    LcmSubscriberSystem,\n    MakeMultibodyStateToWsgStateSystem,\n    MakeRenderEngineVtk,\n    RenderEngineVtkParams,\n    Meshcat,\n    MeshcatPointCloudVisualizer,\n    ModelDirective,\n    ModelDirectives,\n    MultibodyPlant,\n    MultibodyPlantConfig,\n    OutputPort,\n    Parser,\n    PassThrough,\n    ProcessModelDirectives,\n    RgbdSensor,\n    RigidTransform,\n    SceneGraph,\n    SchunkWsgDriver,\n    SchunkWsgPositionController,\n    SchunkWsgCommandSender,\n    SchunkWsgStatusReceiver,\n    ScopedName,\n    SimulatorConfig,\n    StateInterpolatorWithDiscreteDerivative,\n    VisualizationConfig,\n    ZeroForceDriver,\n    DepthRenderCamera,\n)\nfrom pydrake.common.yaml import yaml_load_typed\nfrom drake import (\n    lcmt_iiwa_command,\n    lcmt_iiwa_status,\n    lcmt_schunk_wsg_command,\n    lcmt_schunk_wsg_status,\n)\n\nif running_as_notebook:\n    mpld3.enable_notebook()","block_group":"05a9886d506740118a50a6b276ba946b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1702704260442,"execution_millis":103,"deepnote_to_be_reexecuted":false,"cell_id":"05314cfa8ebe47068881a3b948109c64","deepnote_cell_type":"code"},"source":"from pydrake.all import StartMeshcat\nfrom pydrake.examples import StabilizingLQRController\ncurrent_time = None\nmeshcat = StartMeshcat()","block_group":"2474534f76f9463ab830e496e206f2c8","execution_count":null,"outputs":[{"name":"stderr","text":"INFO:drake:Meshcat listening for connections at https://d095d7c3-1038-4cc5-bfcb-77f17ef5665a.deepnoteproject.com/7000/\n","output_type":"stream"},{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Meshcat URL: <a href='https://d095d7c3-1038-4cc5-bfcb-77f17ef5665a.deepnoteproject.com/7000/' target='_blank'>https://d095d7c3-1038-4cc5-bfcb-77f17ef5665a.deepnoteproject.com/7000/</a>"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c06445fb72084b0485696bfe36f5f6bc","deepnote_cell_type":"text-cell-h1"},"source":"# Finding the Intersection Point Between Drone and Pendulum","block_group":"60a29631705a424e86a9f783c9c93dfd"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2fe381c1509f408b98ede4e69073ce2f","deepnote_cell_type":"text-cell-p"},"source":"In this section, we focus on motion planning and retrieving the final point where the drone flies through a pendulum hoop at a designated time. For this, we have three sections: Estimating the desired point for the quadrotor to go to through modeling the dynamics of the pendulum, tracking the location of the pendulum to verify the system dynamics, and using a Kalman filter to mesh together both of those pieces of information.","block_group":"3e247289bd374ca4a79feb738e35b02d"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4687fa2ad85041749c5e25185d46db68","deepnote_cell_type":"text-cell-h2"},"source":"## Estimating Pendulum Position through Dynamics Modeling","block_group":"aa46121a552e4233b6f48b5223da3175"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1702704394102,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"21f627fc661d4c5787a1625aca71d6f1","deepnote_cell_type":"code"},"source":"g = 9.8 # gravity constant\nl = 1.0 # length of the pendulum\nb = 0.0 # damping factor for the pendulum\nmax_angle = np.radians(90) # maximum angle the pendulum can rise to\n\n# This function models the pendulum equation through the Lagrangian Derivation\ndef damped_pendulum_equation(theta, t, g, l, b):\n    dtheta_dt = theta[1]\n    d2theta_dt2 = -b * theta[1] - (g / l) * np.sin(theta[0])\n    return [dtheta_dt, d2theta_dt2]\n\ninitial_conditions = [max_angle, 0.0] # theta, angular velocity\n\n# This function solves the ODE modeling the pendulum to retrieve the theta at time t \ndef get_theta_at_time(t):\n    t = np.linspace(0, t, 2)\n    return odeint(damped_pendulum_equation, initial_conditions, t, args=(g, l, b))[:, 0][-1]\n","block_group":"19822b5ec9c345348a4b89b1a0dde616","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1702704396983,"execution_millis":13,"is_output_hidden":false,"deepnote_to_be_reexecuted":false,"deepnote_app_is_output_hidden":true,"cell_id":"ce3fbbf72249441092f80fa1099f52b3","deepnote_cell_type":"code"},"source":"# Given a designated time, this function retrieves the position of the pendulum at that time\n\ndef get_coords_at_time(length, t):\n    theta_at_time = get_theta_at_time(t)\n    x = length * np.cos(theta_at_time)\n    if theta_at_time < 0:\n        x = -x\n    y = -length * np.abs(np.sin(theta_at_time))\n    z = dist_from_drone\n    return [x, z, y]","block_group":"6e68c6ed2f00485fa0819fdaf3e2ce9c","execution_count":null,"outputs":[{"name":"stdout","text":"[0.26289580876382973, 0.1, -0.7555698470256816]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e652a94cd191470b87764e456f827595","deepnote_cell_type":"text-cell-h1"},"source":"# Trajectory Optimization/Control","block_group":"cd2fae4d77d84b898aa44b1bfc3df4c8"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7365ce5bd9ce42beb90274b0c133a437","deepnote_cell_type":"text-cell-p"},"source":"This section is all about controlling the drone to drive to the designated point, and also includes the code to render the pendulum in Meshcat.","block_group":"f3a702c8eaa14f53b8fed8d769082b30"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"947c5ec74d664e0d937d21244afe7f9f","deepnote_cell_type":"text-cell-h2"},"source":"## Using LQR Controller to Drive Drone to a Designated Point","block_group":"46658b3c9a27480090ee0d5ce4337034"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3a3a1da459914e1cbcb5d16361c4e4fe","deepnote_cell_type":"text-cell-p"},"source":"This controller uses LQR and has no inclusion of timing. It is used as a preliminary method used, and is used in our baseline, which does not consider the time of intersection between the drone and the pendulum hoop, and simply drives to the designated point immediately after spawning.","block_group":"451b406e6eb94c27ac8b5e93577c0232"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1702704416303,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"b7e42602fff14eaf804208f654c906f5","deepnote_cell_type":"code"},"source":"from pydrake.math import RotationMatrix\n\ndef simulation(x):\n\n    # This simulation contains a swinging pendulum and drives the drone to\n    # designated point specified through the methods above\n\n    builder = DiagramBuilder()\n\n    pendulum = builder.AddSystem(PendulumPlant())\n    plant = builder.AddSystem(QuadrotorPlant())\n\n    # Quadrotor Controller\n    controller = builder.AddSystem(StabilizingLQRController(plant,x))\n    builder.Connect(controller.get_output_port(0), plant.get_input_port(0))\n    builder.Connect(plant.get_output_port(0), controller.get_input_port(0))\n\n    # Setup visualization\n    scene_graph = builder.AddSystem(SceneGraph())\n    PendulumGeometry.AddToBuilder(\n        builder, pendulum.get_state_output_port(), scene_graph\n    )\n    QuadrotorGeometry.AddToBuilder(\n        builder, plant.get_output_port(0), scene_graph\n    )\n    MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n    meshcat.Delete()\n    meshcat.ResetRenderMode()\n    meshcat.SetProperty(\"/Background\", \"visible\", False)\n \n    # Setup slider input for pendulum input torque\n    meshcat.AddSlider(\"u\", min=-5, max=5, step=0.1, value=0.0)\n    torque_system = builder.AddSystem(MeshcatSliders(meshcat, [\"u\"]))\n    builder.Connect(torque_system.get_output_port(), pendulum.get_input_port())\n\n    diagram = builder.Build()\n\n    # Set up a simulator to run this diagram\n    simulator = Simulator(diagram)\n    context = simulator.get_mutable_context()\n\n    pend_context = pendulum.GetMyMutableContextFromRoot(context)\n    params = pendulum.get_mutable_parameters(context=pend_context)\n    params.set_damping(damping=0)\n    params.set_length(length=3)\n\n    meshcat.AddButton(\"Stop Simulation\")\n\n    # Set the initial conditions for an array with the pendulum initial state and the quadrotor initial state\n    init_state = np.concatenate((np.array([np.pi/2, 0]), 0.5 * np.random.randn(12,)), axis = 0)\n    context.SetContinuousState(\n            init_state\n        )\n\n    if running_as_notebook:\n        simulator.set_target_realtime_rate(1.0)\n\n        print(\"Use the slider in the MeshCat controls to apply elbow torque.\")\n        print(\"Press 'Stop Simulation' in MeshCat to continue.\")\n        while meshcat.GetButtonClicks(\"Stop Simulation\") < 1:\n            simulator.AdvanceTo(simulator.get_context().get_time() + 1.0)\n    else:\n        simulator.AdvanceTo(0.1)\n\n    meshcat.DeleteAddedControls()\n\n#This function drives the drone to intersect with the hoop using LQR\n\ndef lqr_drive(t):\n    pend_pos = get_coords_at_time(t)\n    drone_desired_pose = pend_pos\n    simulation(drone_desired_pose)","block_group":"8e72760249644e5888cd18fb0350c5f1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"503621ebe54246798a2f7acc34210882","deepnote_cell_type":"text-cell-h2"},"source":"## Visual Pose Estimation","block_group":"f9d85e7e89ce45c6b40a16967685921d"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7daccc6ab4c4443c99abc311e435bb79","deepnote_cell_type":"text-cell-p"},"source":"Using the cameras in the simulation, we extract RGB images and run some basic feature extraction techniques to extract the large red rectangular contour representing the pendulum rod. The orientation of this contour is used to inform the pendulum's current angle. ","block_group":"47d18881340f4af296b315138028192d"},{"cell_type":"code","metadata":{"cell_id":"fa4b4384268b49db9dc1793c420600e4","deepnote_cell_type":"code"},"source":"class pose_estimation():\n    def __init__(self):\n        meshcat.Delete()\n        meshcat.DeleteAddedControls()\n        meshcat.ResetRenderMode()\n        meshcat.SetProperty(\"/Background\", \"visible\", False)\n\n        builder = DiagramBuilder()\n        \n        # set up the system of manipulation station\n        self.station = MakeManipulationStation(filename=\"package://models/sim.dmd.yaml\", package_xmls=[\"models/package.xml\"]) \n\n        builder.AddSystem(self.station)\n        self.plant = self.station.GetSubsystemByName(\"plant\")\n        \n        params = MeshcatVisualizerParams()\n        params.delete_on_initialization_event = False\n        self.visualizer = MeshcatVisualizer.AddToBuilder(builder, self.station.GetOutputPort(\"query_object\"), meshcat, params)\n\n        scene_graph = builder.AddSystem(SceneGraph())\n        MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n\n        self.diagram = builder.Build()\n        self.world_frame = self.plant.world_frame()\n        context = self.CreateDefaultContext()\n        \n        #################### CAMERA ########################\n        diagram_context = self.station.CreateDefaultContext() #get the context for the entire manipulation station. \n\n        depth_im_read = self.station.GetOutputPort(\"camera0_depth_image\").Eval(diagram_context).data.squeeze() #change to camera_context \n        self.depth_im = deepcopy(depth_im_read)\n        self.depth_im[self.depth_im == np.inf] = 10.0\n        self.rgb_im = self.station.GetOutputPort(\"camera0_rgb_image\").Eval(diagram_context).data\n\n        self.cam = self.station.GetSubsystemByName(\"camera0\")\n        cam_context = self.cam.GetMyMutableContextFromRoot(context)\n        self.X_WC = self.cam.body_pose_in_world_output_port().Eval(cam_context)\n        self.X_WC = RigidTransform(self.X_WC)\n        self.cam_info = self.cam.depth_camera_info()\n\n    def CreateDefaultContext(self):\n        context = self.diagram.CreateDefaultContext()\n        plant_context = self.diagram.GetMutableSubsystemContext(self.plant, context)\n        station_context = self.diagram.GetMutableSubsystemContext(self.station, context)\n        return context\n\n    def viz(self, sim_duration=1.0):\n        # Set up a simulator to run this diagram\n        simulator = Simulator(self.diagram)\n        simulator.Initialize()\n        if running_as_notebook:\n            simulator.set_target_realtime_rate(1.0)\n            simulator.AdvanceTo(sim_duration)\n        else:\n            simulator.AdvanceTo(0.1)\n \n# Find the orientation of the extracted contour\ndef getOrientation(pts, img):\n  sz = len(pts)\n  data_pts = np.empty((sz, 2), dtype=np.float64)\n  for i in range(data_pts.shape[0]):\n    data_pts[i,0] = pts[i,0,0]\n    data_pts[i,1] = pts[i,0,1]\n \n  mean = np.empty((0))\n  mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)\n \n  cntr = (int(mean[0,0]), int(mean[0,1]))\n\n  p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])\n  p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])\n  angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians\n\n  return angle","block_group":"1ed3135776f14aa8a4c2d86a4b11a243","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"602a9710a4964535940af468ec336d79","deepnote_cell_type":"code"},"source":"# Query every second for 20 seconds to capture a few passes of the pendulum\ncam_angles_obs = []\nfor i in range(20):\n    pend = pose_estimation()\n    im = pend.rgb_im\n    image = cv2.cvtColor(im, cv2.COLOR_RGB2HSV)\n    mask1 = cv2.inRange(image, np.array([0, 70, 50]), np.array([10, 255, 255]))\n    mask2 = cv2.inRange(image, np.array([160, 70, 50]), np.array([180, 255, 255]))\n    mask = mask1 + mask2\n\n    output_img = im.copy()\n\n    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n\n    for c in cnts:\n        x,y,w,h = cv2.boundingRect(c)\n        cv2.rectangle(im, (x, y), (x + w, y + h), [0,0,0], 2)\n\n    for i, c in enumerate(cnts):\n        area = cv2.contourArea(c)\n        if area < 100 or 5000 < area:\n            continue\n        ang = getOrientation(c, output_img)\n    \n    plt.imshow(output_img)\n    plt.show()  \n\n    pend.viz()\n    cam_angles_obs.append(int(np.rad2deg(ang)) - 90))","block_group":"bda9f1e26cd84631b4f5cab70768a759","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d96a4d1cb668490eb6fd55a40dcba262","deepnote_cell_type":"text-cell-h2"},"source":"## Kalman Filtering","block_group":"92ecb1b0c5d34c609dfb8c965f3b72b6"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cffe6c08643c4718af236988549e1cf7","deepnote_cell_type":"text-cell-p"},"source":"To combine the outputs of the dynamics model and the visual observations from the camera we add a Kalman filter into the pipeline. We can then query for the final desired positions from these filtered values (representing an updated understanding of the pendulum dynamics).","block_group":"8dfac0b41714471087d532d4ad05991d"},{"cell_type":"code","metadata":{"cell_id":"8a23450b2d374bd5b1fb1381497202eb","deepnote_cell_type":"code"},"source":"# Time points to evaluate\nt = np.linspace(0, 20, 1000)  # Time from 0 to 20 seconds with 1000 points\n\n# Solve the damped pendulum ODE\nsolution = odeint(damped_pendulum_equation, initial_conditions, t, args=(g, l, b))\n\n# Extract theta values from the solution\ntheta_values = solution[:, 0]\n\ncam_t = np.linspace(0, 19, 20)\n\n# Plot the results of the dynamics model (blue line), camera observations (orange points) and filtered output (orange line)\nplt.plot(t, np.degrees(theta_values))\nplt.scatter(cam_t, cam_angles_obs, c=\"red\")\nplt.title('Pendulum Motion')\nplt.xlabel('Time (s)')\nplt.ylabel('Angular Displacement (degrees)')\nplt.grid(True)\nplt.show()","block_group":"773f8d215be749cb94a8aacfa4ef2c0c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d9e510e9d4a64279beab70bb6747295c","deepnote_cell_type":"text-cell-h2"},"source":"## Direct Collocation","block_group":"ef672a66db94481fa70f05abeca2c702"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9faa21d60c374ca688444daaa6e2d048","deepnote_cell_type":"text-cell-p"},"source":"The next method is direct collocation, a numerical optimization technique used in trajectory optimization for dynamic systems, to determine its trajectory. It formulates the problem by discretizing the system dynamics and the control inputs over a finite set of collocation points along the trajectory. It simultaneously optimizes both the state variables and control inputs at these collocation points. This method is used to plan the drone's motion, and the produced trajectory is then fed into the simulator so it can be rendered using the given quadrotor.","block_group":"0abc21d7d57e4bc19187287bd4e69df7"},{"cell_type":"code","metadata":{"cell_id":"58e5ba3288294743b223ef227213b859","deepnote_cell_type":"code"},"source":"quadrotor = \"\"\"\n<?xml version=\"1.0\"?>\n<!-- Mesh file and approximate numbers are from Abe Bachrach at Skdio.  -->\n<robot name=\"quadrotor\">\n  <!--\n  Axes are standard vehicle coordinates:\n    +X - Pointing toward the nose (primary camera).\n    +Y - Towards the left rotors.\n    +Z - Towards the top of the vehicle..\n  Origin:\n    (0, 0, 0) in the center of the main quadrotor body.\n  -->\n  <link name=\"base_link\">\n    <inertial>\n      <mass value=\"0.775\"/>\n      <origin xyz=\"0 1 0\"/>\n      <inertia ixx=\"0.0015\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.0025\" iyz=\"0.0\" izz=\"0.0035\"/>\n    </inertial>\n    <visual>\n      <origin rpy=\"1.570796 0 0\" xyz=\"0 1 0\"/>\n      <geometry>\n        <mesh filename=\"package://drake_models/skydio_2/skydio_2_1000_poly.obj\" scale=\".00254\"/>\n      </geometry>\n    </visual>\n    <collision>\n      <origin rpy=\"0 1 0\" xyz=\"0 0 0\"/>\n      <geometry>\n        <box size=\".36 .4 .06\"/>\n      </geometry>\n    </collision>\n  </link>\n</robot>\n\"\"\"","block_group":"969b589fae4d4725bea7ca8f2e051453","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"06b2e8b1bb524b969265cf2f1c96e83f","deepnote_cell_type":"code"},"source":"def dircol_quadrotor(final_position, time):\n    # Outputs the final trajectory necessary to drive the quadrotor to the final state\n    plant = QuadrotorPlant()\n    context = plant.CreateDefaultContext()\n\n    dircol = DirectCollocation(\n        plant,\n        context,\n        num_time_samples=21,\n        minimum_time_step= time/20.0, # Used to control the time at which the pendulum and drone should intersect\n        maximum_time_step=0.2,\n    )\n    prog = dircol.prog()\n\n    dircol.AddEqualTimeIntervalsConstraints()\n\n    # Add input torque limit\n    torque_limit = 8.0  # N*m.\n    u = dircol.input()\n    dircol.AddConstraintToAllKnotPoints(-torque_limit <= u[0])\n    dircol.AddConstraintToAllKnotPoints(u[0] <= torque_limit)\n    \n    # Begins at position point [0, 0, 0] with zero velocity\n    initial_state = np.zeros((12,1))\n    prog.AddBoundingBoxConstraint(\n        initial_state, initial_state, dircol.initial_state()\n    )\n   \n    # Final state determined by stationary end position of quadrotor\n    final_state = [final_position[0], final_position[1], final_position[2], 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    prog.AddBoundingBoxConstraint(\n        final_state, final_state, dircol.final_state()\n    )\n    \n    R = 10  # Cost on input \"effort\".\n    dircol.AddRunningCost(R * u[0] ** 2)\n\n    # Add a final cost equal to the total duration.\n    dircol.AddFinalCost(dircol.time())\n\n    # Give an initial guess from a solution.\n    initial_x_trajectory = PiecewisePolynomial.FirstOrderHold(\n        [0.0, 0.2, 1.4, 2.0, 4.0],\n        np.column_stack(\n            (\n                initial_state,\n                [0,0,0,0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0],\n                final_state,\n            )\n        ),\n    )\n  \n    dircol.SetInitialTrajectory(PiecewisePolynomial(), initial_x_trajectory)\n\n    solver = SnoptSolver()\n    solver_id = solver.solver_id()\n    major_tol = 1e-3\n    minor_tol = 1e-3\n    prog.SetSolverOption(solver_id, \"Feasibility tolerance\", major_tol)\n    prog.SetSolverOption(solver_id, \"Major feasibility tolerance\", major_tol)\n    prog.SetSolverOption(solver_id, \"Major optimality tolerance\", major_tol)\n    prog.SetSolverOption(solver_id, \"Minor feasibility tolerance\", minor_tol)\n    prog.SetSolverOption(solver_id, \"Minor optimality tolerance\", minor_tol)\n    result = Solve(prog)\n    assert result.is_success()\n    \n    # Generating trajectory of input torques\n    u_trajectory = dircol.ReconstructInputTrajectory(result)\n    times = np.linspace(\n        u_trajectory.start_time(), u_trajectory.end_time(), 100\n    )\n    u_values = u_trajectory.vector_values(times)\n     \n    # Generating trajectory of states\n    x_trajectory = dircol.ReconstructStateTrajectory(result)\n    x_values = x_trajectory.vector_values(times)\n\n    return TrajectorySource(x_trajectory)\n\n\nx_traj_source = dircol_quadrotor()","block_group":"e7bc84cc49424710abc6dabbb62e38da","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"caa5648494604432b63a72727976b2cc","deepnote_cell_type":"text-cell-h3"},"source":"### Meshcat Simulator Rendering","block_group":"df8531302fc34ba396caf926bf143e84"},{"cell_type":"code","metadata":{"cell_id":"35c5159041f844a3a24fbfc467350355","deepnote_cell_type":"code"},"source":"def create_scene(x_traj_source, sim_time_step=0.0001):\n    meshcat.Delete()\n    meshcat.DeleteAddedControls()\n\n    # Adding quadrotor to diagram\n    builder = DiagramBuilder()\n    plant = MultibodyPlant(time_step=0.0)\n    scene_graph = SceneGraph()\n    plant.RegisterAsSourceForSceneGraph(scene_graph)\n    parser = Parser(plant)\n    parser.AddModelsFromString(quadrotor, \"urdf\") # loaded from the quadrotor model above\n    plant.Finalize()\n   \n    plant_context = plant.CreateDefaultContext()\n\n    # Note: The quadrotor model loaded from the urdf has a 13-element state because quaternions are used \n    # for orientation instead of rpy\n    plant.SetPositions(plant_context, np.zeros((7,1))) # Set all states to 0\n    plant.SetVelocities(plant_context, np.zeros((6,1))) # Set all states to 0\n\n    builder.AddSystem(x_traj_source)\n    builder.AddSystem(scene_graph)\n    QuadrotorGeometry.AddToBuilder(builder, x_traj_source.get_output_port(0), scene_graph)\n    pos_to_pose = builder.AddSystem(\n        MultibodyPositionToGeometryPose(plant, input_multibody_state=True)\n    )\n\n    # Adding pendulum to diagram\n    pendulum = PendulumPlant()\n    PendulumGeometry.AddToBuilder(\n        builder, pendulum.get_state_output_port(), scene_graph\n    )\n\n    # Adding top-view (cam0) and frontal (cam1) cameras to diagram\n    camera_files = [\"models/cam0.sdf\", \"models/cam1.sdf\"]\n    for i in range(len(camera_files)):\n        cam_plant = MultibodyPlant(time_step=0.0)\n        cam_scene_graph = SceneGraph()\n        cam_plant.RegisterAsSourceForSceneGraph(cam_scene_graph)\n        parser = Parser(cam_plant)\n        parser.AddModels(camera_files[i]) \n        builder.AddSystem(cam_scene_graph)\n\n        renderer = None\n        depth_camera = None\n\n        if not renderer:\n            renderer = \"my_renderer\"\n\n        if not scene_graph.HasRenderer(renderer):\n            scene_graph.AddRenderer(renderer, MakeRenderEngineVtk(RenderEngineVtkParams()))\n\n        if not depth_camera:\n            depth_camera = DepthRenderCamera(RenderCameraCore(renderer, CameraInfo(width=640, height=480, fov_y=np.pi / 4.0), ClippingRange(near=0.1, far=10.0), RigidTransform()),DepthRange(0.1, 10.0))\n\n        inspector = scene_graph.model_inspector()\n        ids = inspector.GetAllFrameIds()\n\n        rgbd = builder.AddSystem(RgbdSensor(parent_id = ids[0], X_PB=RigidTransform(), depth_camera=depth_camera, show_window=False))\n        model_name = \"cam\"+str(i)\n        rgbd.set_name(model_name)\n        builder.Connect(scene_graph.get_query_output_port(),rgbd.query_object_input_port())\n\n        # Export the camera outputs\n        builder.ExportOutput(rgbd.color_image_output_port(), f\"{model_name}_rgb_image\")\n        builder.ExportOutput(rgbd.depth_image_32F_output_port(), f\"{model_name}_depth_image\")\n        builder.ExportOutput(rgbd.label_image_output_port(), f\"{model_name}_label_image\")\n\n    # Setup slider input for pendulum input torque\n    meshcat.AddSlider(\"u\", min=-5, max=5, step=0.1, value=0.0)\n    torque_system = builder.AddSystem(MeshcatSliders(meshcat, [\"u\"]))\n    builder.Connect(torque_system.get_output_port(), pendulum.get_input_port())\n\n    # Add visualizer to visualize the geometries.\n    visualizer = MeshcatVisualizer.AddToBuilder(\n        builder, scene_graph, meshcat)\n\n    diagram = builder.Build()\n    return diagram, visualizer","block_group":"a9697ee99feb42be939e8034855584b1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"1fbce3bcac4149638572cfcbcedf7284","deepnote_cell_type":"code"},"source":"def run_simulation(x_traj_source, sim_time_step):\n    diagram, visualizer = create_scene(x_traj_source, sim_time_step)\n    simulator = initialize_simulation(diagram)\n    visualizer.StartRecording()\n    simulator.AdvanceTo(2)\n    visualizer.PublishRecording()\n\nrun_simulation(x_traj_source, 0.0001)","block_group":"46ecf8444d7c4e14a2b6b67686d331f9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8f6eeedbc0d041128243a0596a667a48","deepnote_cell_type":"text-cell-h1"},"source":"# Demos","block_group":"77ee8d721185451c8ec6ede39bf82a34"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9be2397ed7864500aa983ac3c6cc275b","deepnote_cell_type":"text-cell-p"},"source":"The following links contain demos of the final drone simulation","block_group":"87d415eb48cc4693beb85779cfb2b852"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"url":"https://youtu.be/Yer-eJaSbwc","type":"link","ranges":[],"toCodePoint":41,"fromCodePoint":13}],"cell_id":"0700e40b91c5485ca11b522f9817e2e9","deepnote_cell_type":"text-cell-p"},"source":"Demo link 1: https://youtu.be/Yer-eJaSbwc","block_group":"6b1f9310e23a417cbbc8e167e14d36c5"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"url":"https://youtu.be/RP1e5ff_Fc8","type":"link","ranges":[],"toCodePoint":41,"fromCodePoint":13}],"cell_id":"16fc0cabd7604207856d1d1463c6ad7d","deepnote_cell_type":"text-cell-p"},"source":"Demo link 2: https://youtu.be/RP1e5ff_Fc8","block_group":"031f337047904e3b884bb8b6375945f7"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d095d7c3-1038-4cc5-bfcb-77f17ef5665a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"5229b3626600429b800519b4692babeb","deepnote_execution_queue":[]}}